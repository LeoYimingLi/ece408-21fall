{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 // MP Reduction\
// Given a list (lst) of length n\
// Output its sum = lst[0] + lst[1] + ... + lst[n-1];\
\
#include <wb.h>\
\
#define BLOCK_SIZE 512 //@@ You can change this\
\
#define wbCheck(stmt)                                                     \\\
  do \{                                                                    \\\
    cudaError_t err = stmt;                                               \\\
    if (err != cudaSuccess) \{                                             \\\
      wbLog(ERROR, "Failed to run stmt ", #stmt);                         \\\
      wbLog(ERROR, "Got CUDA error ...  ", cudaGetErrorString(err));      \\\
      return -1;                                                          \\\
    \}                                                                     \\\
  \} while (0)\
  \
__global__ void total(float *input, float *output, int len) \{\
  // len == numInputElements\
  //@@ Load a segment of the input vector into shared memory\
  //@@ Traverse the reduction tree\
  //@@ Write the computed sum of the block to the output vector at the\
  //@@ correct index\
  \
  // shared memory\
  __shared__ float partialSum[BLOCK_SIZE<<1];\
  unsigned int t = threadIdx.x;  // index of partialSum\
  unsigned int bx = blockIdx.x;  // index of output\
  unsigned int start = 2*blockIdx.x*blockDim.x;  // the coverage before this thread\
  \
  unsigned int idx1 = start + t;\
  unsigned int idx2 = start + t + blockDim.x;\
  \
  // load data into partialSum[]\
  if (idx1<len) \{\
    partialSum[t] = input[idx1];\
  \} else \{\
    partialSum[t] = 0;\
  \}\
  \
  if (idx2<len) \{\
    partialSum[t + blockDim.x] = input[start + blockDim.x + t];\
  \} else \{\
    partialSum[t + blockDim.x] = 0;\
  \}\
  \
  // traverse the reduction tree:\
  for (unsigned int stride = blockDim.x; stride >= 1; stride >>= 1) \{\
    __syncthreads();\
    if (t < stride)  // some threads are made inactive by this;\
      partialSum[t] += partialSum[t+stride];\
  \}\
  \
  if (t == 0) \{  // but this can cause branch divergence..\
    output[bx] = partialSum[0];\
  \}\
\}\
\
int main(int argc, char **argv) \{\
  int ii;\
  wbArg_t args;\
  float *hostInput;  // The input 1D list\
  float *hostOutput; // The output list\
  float *deviceInput;\
  float *deviceOutput;\
  int numInputElements;  // number of elements in the input list\
  int numOutputElements; // number of elements in the output list\
\
  args = wbArg_read(argc, argv);\
\
  wbTime_start(Generic, "Importing data and creating memory on host");\
  hostInput =\
      (float *)wbImport(wbArg_getInputFile(args, 0), &numInputElements);\
  \
  // numOutputElements is the number of blocks\
  numOutputElements = numInputElements / (BLOCK_SIZE << 1);\
  if (numInputElements % (BLOCK_SIZE << 1)) \{\
    numOutputElements++;\
  \}\
  hostOutput = (float *)malloc(numOutputElements * sizeof(float));\
\
  wbTime_stop(Generic, "Importing data and creating memory on host");\
\
  wbLog(TRACE, "The number of input elements in the input is ",\
        numInputElements);\
  wbLog(TRACE, "The number of output elements in the input is ",\
        numOutputElements);\
\
  wbTime_start(GPU, "Allocating GPU memory.");\
  //@@ Allocate GPU memory here\
  cudaMalloc((void **) &deviceInput, numInputElements * sizeof(float));\
  cudaMalloc((void **) &deviceOutput, numOutputElements * sizeof(float));\
  \
\
  wbTime_stop(GPU, "Allocating GPU memory.");\
\
  wbTime_start(GPU, "Copying input memory to the GPU.");\
  //@@ Copy memory to the GPU here\
  cudaMemcpy(deviceInput, hostInput, numInputElements * sizeof(float), cudaMemcpyHostToDevice);\
  \
\
  wbTime_stop(GPU, "Copying input memory to the GPU.");\
  //@@ Initialize the grid and block dimensions here\
  // dim3 DimGrid(ceil(1.0*numCColumns/TILE_WIDTH), 1, 1);\
  dim3 DimGrid(numOutputElements, 1, 1);\
  dim3 DimBlock(BLOCK_SIZE, 1, 1);\
  \
  wbTime_start(Compute, "Performing CUDA computation");\
  //@@ Launch the GPU Kernel here\
  total<<<DimGrid, DimBlock>>>(deviceInput, deviceOutput, numInputElements);\
  \
  cudaDeviceSynchronize();\
  wbTime_stop(Compute, "Performing CUDA computation");\
\
  wbTime_start(Copy, "Copying output memory to the CPU");\
  //@@ Copy the GPU memory back to the CPU here\
  cudaMemcpy(hostOutput, deviceOutput, numOutputElements * sizeof(float), cudaMemcpyDeviceToHost);\
\
  wbTime_stop(Copy, "Copying output memory to the CPU");\
\
  /********************************************************************\
   * Reduce output vector on the host\
   * NOTE: One could also perform the reduction of the output vector\
   * recursively and support any size input. For simplicity, we do not\
   * require that for this lab.\
   ********************************************************************/\
  for (ii = 1; ii < numOutputElements; ii++) \{\
    hostOutput[0] += hostOutput[ii];\
  \}\
\
  wbTime_start(GPU, "Freeing GPU Memory");\
  //@@ Free the GPU memory here\
  cudaFree(deviceInput);\
  cudaFree(deviceOutput);\
\
  wbTime_stop(GPU, "Freeing GPU Memory");\
\
  wbSolution(args, hostOutput, 1);\
\
  free(hostInput);\
  free(hostOutput);\
\
  return 0;\
\}\
}